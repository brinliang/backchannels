{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This basic implementation for verbal backchannels follows the following procedure:\n",
    "- listens for audio input\n",
    "- waits for audio input to pause\n",
    "- convert audio chunk to text\n",
    "- generate response from text\n",
    "- convert response to audio\n",
    "- play audio\n",
    "\n",
    "Implementation notes:\n",
    "- there is very high latency between each phrase and the returned backchannel which is an issue for maintaining conversational pace\n",
    "- each cell in this notebook is labeled with a description of its function and can be run independently\n",
    "\n",
    "Experimentation areas:\n",
    "- speech to text systems\n",
    "  - google (current)\n",
    "  - assembly ai\n",
    "  - whisper (very good but larger models can be slow)\n",
    "- prompts for generating responses\n",
    "  - current: 'Respond with a verbal backchannel as if you are actively listening to someone say \"{user input}\"'\n",
    "- text to audio systems\n",
    "  - pyttsx3 (current)\n",
    "  - eleven ai (very human-like but not real time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "import openai\n",
    "import time\n",
    "import config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech to Text\n",
    "First, we use SpeechRecognition to detect spoken phrases and transcribe them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sr.Recognizer() # recognizer instance\n",
    "m = sr.Microphone() # audio input instance\n",
    "# if this part isn't working, try printing m.list_microphone_names() and setting sr.Microphone(device_index) to the microphone you want to use\n",
    "\n",
    "# set audio input threshold and calibrate\n",
    "r.energy_threshold = 1000\n",
    "r.dynamic_energy_threshold = False\n",
    "with m:\n",
    "  r.adjust_for_ambient_noise(m)\n",
    "\n",
    "\n",
    "# set pause between phrases threshold, default is 0.8\n",
    "# r.pause_threshold = 0.8\n",
    "\n",
    "while True:\n",
    "  print('listening...')\n",
    "  # this works by listening for an audio input, waiting for a pause, then returning the audio chunk\n",
    "  with m:\n",
    "    audio = r.listen(m) \n",
    "\n",
    "  print('audio received. transcribing...')\n",
    "  # use google speech to text to transcribe audio\n",
    "  try:\n",
    "    print(r.recognize_google(audio))\n",
    "  except:\n",
    "    print('failed to transcribe audio')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Backchannel\n",
    "Then we prompt gpt3 with the transcribed text for a backchannel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I layed out the variables in this way, but just change gpt3_input to the string you want to prompt gpt3 with. \n",
    "prompt = 'Respond with a verbal backchannel as if you are actively listening to someone say \"{}\"'\n",
    "user_input = 'Your backchannels are a bit delayed'\n",
    "gpt3_input = prompt.format(user_input)\n",
    "\n",
    "# I used the default parameters, but these can be changed. \n",
    "# More details available at https://platform.openai.com/docs/api-reference/completions.\n",
    "openai.api_key = config.api_key\n",
    "result = openai.Completion.create(\n",
    "    model='text-davinci-003',\n",
    "    prompt=gpt3_input,\n",
    "    max_tokens=256,\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Text\n",
    "Then we play the text with pyttsx3 (turn volume on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = pyttsx3.init()  # initialize pyttsx3 instance\n",
    "engine.say('text to say')  # say something\n",
    "engine.runAndWait() # clean up\n",
    "engine.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Putting everything together, we get this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = config.api_key # set api key\n",
    "\n",
    "r = sr.Recognizer() # recognizer instance\n",
    "m = sr.Microphone() # audio input instance\n",
    "engine = pyttsx3.init() # audio output instance\n",
    "\n",
    "# prompt for gpt3 input\n",
    "prompt = 'Respond with a verbal backchannel as if you are actively listening to someone say \"{}\"'\n",
    "\n",
    "# set audio input threshold and calibrate\n",
    "r.energy_threshold = 1000\n",
    "r.dynamic_energy_threshold = False\n",
    "with m:\n",
    "  r.adjust_for_ambient_noise(m)\n",
    "\n",
    "# set pause between phrases threshold, default is 0.8 seconds\n",
    "# r.pause_threshold = 0.8\n",
    "\n",
    "while True:\n",
    "  # listen for audio input\n",
    "  start = time.time()\n",
    "  print('listening...')\n",
    "  with m:\n",
    "    audio = r.listen(m)\n",
    "  print('audio received after {0:.4f} seconds'.format(time.time() - start))\n",
    "\n",
    "  # transcribe\n",
    "  try: \n",
    "    lap = time.time()\n",
    "    print('\\ntranscribing...')\n",
    "    transcript = r.recognize_google(audio)\n",
    "    print(transcript)\n",
    "    print('transcribed in {0:.4f} seconds'.format(time.time() - lap))\n",
    "  except:\n",
    "    print('failed to transcribe\\nplease try again')\n",
    "    continue\n",
    "\n",
    "  # generate response\n",
    "  try:\n",
    "    lap = time.time()\n",
    "    print('\\ngenerating response...')\n",
    "    gpt3_input = prompt.format(transcript)\n",
    "    gpt3_output = openai.Completion.create(\n",
    "      model='text-davinci-003',\n",
    "      prompt=gpt3_input,\n",
    "      max_tokens=256,\n",
    "    )\n",
    "    print(gpt3_output)\n",
    "    response = gpt3_output['choices'][0]['text'].strip()\n",
    "    print('generated in {0:.4f} seconds'.format(time.time() - lap))\n",
    "  except:\n",
    "    print('failed to generate response\\ncheck to make sure your api key is valid')\n",
    "    continue\n",
    "\n",
    "  # play response\n",
    "  lap = time.time()\n",
    "  print('\\nplaying response...')\n",
    "  print(response)\n",
    "  engine.say(response)\n",
    "  print('played in {0:.4f} seconds'.format(time.time() - lap))\n",
    "  print('finished in {0:.4f} seconds'.format(time.time() - start))\n",
    "  engine.runAndWait()\n",
    "  engine.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "al3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a95d71271daa848b6c62eaea544795325370f4813672235eeb3633ba81061551"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
